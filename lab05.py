# -*- coding: utf-8 -*-
"""Lab05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ebGqvUO3nnGOdJ9wkwwHkN06su7JTthx
"""

a = 'flying fish flew by the space station'
b = 'he will not allow you to bring your sticks of dynamite and pet armadillo along'
c = 'he figured a few sticks of dynamite youre easier than a fishing pole to catch an armadillo'

def shingle(text, k):
    if k <= 0:
        raise ValueError("k should be a positive integer.")

    # Remove any leading or trailing whitespaces and convert to lowercase
    text = text.strip().lower()

    # Check if the length of the text is less than k
    if len(text) < k:
        raise ValueError("Text length should be greater than or equal to k.")

    # Generate k-shingles
    shingles = set()
    for i in range(len(text) - k + 1):
        shingle = text[i:i + k]
        shingles.add(shingle)

    return shingles

a = shingle(a,3)
b= shingle(b,3)
c= shingle(c,3)

print(a)
print(b)
print(c)

shingle_vocabulary = a | b | c
print(shingle_vocabulary)

# Task 2 hot encoding sparse vectors

a_1hot = [1 if x in a else 0 for x in shingle_vocabulary]
b_1hot = [1 if x in b else 0 for x in shingle_vocabulary]
c_1hot = [1 if x in c else 0 for x in shingle_vocabulary]

print(a_1hot)
print(b_1hot)
print(c_1hot)

from random import shuffle

# task 3 minhashing

def create_hash_function(size: int):

  hash_ex = list(range(1,size+1))
  shuffle(hash_ex)
  return hash_ex


def build_minhash_function(vocab_size: int, nbits: int):
  hashes = []
  for i in range(nbits):
    hashes.append(create_hash_function(vocab_size))

  return hashes

minhash_func = build_minhash_function(len(shingle_vocabulary), 20)

def create_hash(vector: list):
  signature = []
  for func in minhash_func:
    for i in range(1, len(shingle_vocabulary)+1):
      idx = func.index(i)
      signature_val = vector[idx]
      if signature_val == 1:
        signature.append(idx)
        break
  return signature


a_sig = create_hash(a_1hot)
b_sig = create_hash(b_1hot)
c_sig = create_hash(c_1hot)

print(a_sig)
print(b_sig)
print(c_sig)

def jaccard(a: set , b: set):
  return len(a & b)/len(a | b)


jaccard(a,b), jaccard(set(a_sig), set(b_sig))

jaccard (a,c), jaccard(set(a_sig), set(c_sig))

jaccard(b,c), jaccard(set(b_sig), set(c_sig))

# task 4

def split_vector(signature, b):
  assert len(signature) % b == 0
  r = int(len(signature)/b)

  subvecs = []
  for i in range(0, len(signature), r):
    subvecs.append(signature[i : i+r])
  return subvecs

band_b = split_vector(b_sig, 10)
band_b

band_c = split_vector(c_sig,10)
band_c



for b_rows, c_rows in zip(band_b, band_c):
  if b_rows == c_rows:
    print(f"Candidate pair: {b_rows} == {c_rows}")

    break

band_a = split_vector(a_sig,10)
band_a

for a_rows, b_rows in zip(band_a, band_b):
  if a_rows == b_rows:
    print(f"Candidate pair: {a_rows} == {b_rows}")

    break

for a_rows, c_rows in zip(band_a, band_c):
  if a_rows == c_rows:
    print(f"Candidate pair: {a_rows} == {c_rows}")

    break

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def probability(s, r, b):
  return 1 - (1 - s**r)**b


results = pd.DataFrame({
    's' : [],
    'p' : [],
    'r,b' : []
})


for s in np.arange(0.01, 1, 0.01):
  total = 100
  for b in [100, 50, 25, 20, 10, 5, 4, 2, 1]:
    r = int(total/b)
    p = probability(s, r, b)
    results  = results.append({
        's': s,
        'p': p,
        'r,b' : f"{r},{b}"
    }, ignore_index = True)

sns.lineplot(data=results, x = 's', y= 'p', hue = 'r,b')

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
import re
from itertools import combinations
from random import shuffle

# Sample dataset
documents = [
    "The quick brown fox jumps over the lazy dog",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit",
    "The five boxing wizards jump quickly",
    "How vexingly quick daft zebras jump!",
    "Bright vixens jump; dozy fowl quack"
]

# Step 1: Basic Pre-processing
def preprocess_text(text):
    # Convert to lowercase and remove punctuation
    text = re.sub(r'[^\w\s]', '', text.lower())
    return text

preprocessed_documents = [preprocess_text(doc) for doc in documents]

# Step 2: Shingling
def shingle(text, k):
    text = preprocess_text(text)
    shingles = set()
    for i in range(len(text) - k + 1):
        shingle = text[i:i + k]
        shingles.add(shingle)
    return shingles

k_shingles = [shingle(doc, k=3) for doc in documents]

# Step 3: Sparse Vector Representation
shingle_vocabulary = set.union(*k_shingles)

def create_sparse_vector(shingles, vocabulary):
    return [1 if shingle in shingles else 0 for shingle in vocabulary]

sparse_vectors = [create_sparse_vector(shingles, shingle_vocabulary) for shingles in k_shingles]

# Step 4: Minhashing
def create_hash_function(size):
    hash_ex = list(range(1, size + 1))
    shuffle(hash_ex)
    return hash_ex

def build_minhash_function(vocab_size, nbits):
    hashes = []
    for i in range(nbits):
        hashes.append(create_hash_function(vocab_size))
    return hashes

minhash_func = build_minhash_function(len(shingle_vocabulary), 20)

def create_hash(vector):
    signature = []
    for func in minhash_func:
        for i in range(1, len(shingle_vocabulary) + 1):
            idx = func.index(i)
            signature_val = vector[idx]
            if signature_val == 1:
                signature.append(idx)
                break
    return signature

minhash_signatures = [create_hash(vector) for vector in sparse_vectors]

# Step 5: Locality Sensitive Hashing (LSH)
def split_vector(signature, b):
    assert len(signature) % b == 0
    r = int(len(signature) / b)
    subvecs = [tuple(signature[i:i + r]) for i in range(0, len(signature), r)]
    return subvecs

bands = split_vector(minhash_signatures[0], 5)  # Using 5 bands for illustration

candidates = set()

for i in range(len(minhash_signatures)):
    signature = minhash_signatures[i]
    doc_bands = split_vector(signature, 5)  # Using 5 bands for illustration

    for j in range(i + 1, len(minhash_signatures)):
        other_signature = minhash_signatures[j]
        other_bands = split_vector(other_signature, 5)  # Using 5 bands for illustration

        if any(x == y for x, y in zip(doc_bands, other_bands)):
            candidates.add((i, j))

# Step 6: Analysis
def jaccard_similarity(doc1, doc2):
    set1 = k_shingles[doc1]
    set2 = k_shingles[doc2]
    return len(set1.intersection(set2)) / len(set1.union(set2))

# Evaluate candidates
for candidate_pair in candidates:
    doc1, doc2 = candidate_pair
    jaccard_original = jaccard_similarity(doc1, doc2)
    jaccard_minhash = jaccard_similarity(doc1, doc2)
    print(f"Document Pair {doc1 + 1} and {doc2 + 1}:")
    print(f"Jaccard Similarity (Original): {jaccard_original}")
    print(f"Jaccard Similarity (Minhash): {jaccard_minhash}")
    print("-" * 30)

  # ... (Previous code remains unchanged)

# Step 6: Analysis
def print_document_pair_results(doc1, doc2, jaccard_original, jaccard_minhash):
    print(f"Document Pair {doc1 + 1} and {doc2 + 1}:")
    print(f"Jaccard Similarity (Original): {jaccard_original}")
    print(f"Jaccard Similarity (Minhash): {jaccard_minhash}")
    print("-" * 30)

# Evaluate candidates
for candidate_pair in candidates:
    doc1, doc2 = candidate_pair
    jaccard_original = jaccard_similarity(doc1, doc2)
    jaccard_minhash = jaccard_similarity(doc1, doc2)
    print_document_pair_results(doc1, doc2, jaccard_original, jaccard_minhash)